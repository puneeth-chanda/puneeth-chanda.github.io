---
bg: "deep.jpg"
layout: post
title:  "Batch Normalization"
crawlertitle: "Batch Normalization"
summary: "Why and when is batch normalization used"
date:   2020-07-24 20:09:47 +0700
categories: posts
tags: ['Deep learning']
author: puneeth chanda
---

# Why and when is batch normalization used

> Batch normalization provides an elegant way of reparametrizing almost any deep network. The reparametrization significantly reduces the problem of coordinating updates across many layers.

Normalizing the inputs to the layer has an effect on the training of the model, dramatically reducing the number of epochs required. It can also have a regularizing effect, reducing generalization error much like the use of activation regularization.