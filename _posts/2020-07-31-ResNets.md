---
bg: "deep.jpg"
layout: post
title:  "ResNets"
crawlertitle: "Residual Networks"
summary: "What,Why and When are ResNets used"
date:   2020-07-31 20:09:47 +0700
categories: posts
tags: ['Deep learning']
author: puneeth chanda
---

## What are ResNets

So let us start by the diiference between a normal(plain) network and a Residual(ResNet) network.

this is a plain network
![*part*](/assets/images/plain.png)

and this is a residual network
![resnet](/assets/images/resnet.png)

so the difference between these two is in the first layer

**Plain**: The starting activation *a[l]* is passed through a layer and the resulting through another and so on and the output partition we get is *a[l+2]* which is generated only using the activation *a[l+1]* 
![plain](/assets/images/plain_text.png)

**ResNet**: The activation *a[l]* is passed to the last RelU activation skipping the middle later this is to ensure that failure of training a deeper neural network is reduced

![resnet](/assets/images/resnet_text.png)

### When do we use ResNets?

ResNets are used to train deep neural networks to avoid the ***vanishing gradient problem*** - as the gradient is back-propagated to earlier layers, repeated multiplication may make the gradient extremely small, which makes it difficult to train deep neural networks

### Why do they work?

So resnets have **skip connections** which help in flow of information from earlier layers in the model to later layers. These skip connections provide an alternative shortcut way for the gradients to back-propogate.

Resnets allow the model to learn an identity function i.e. due to [batch normalization](/posts/Batch-Normalization/) the parameters of *z[l+2]* i.e. *W* and *b* might become closer to zero so at least the value would be *g[a[l]]*.

![resnet](/assets/images/why_resnet.png)

which makes the network work the same as a earlier layer but not worse. 

